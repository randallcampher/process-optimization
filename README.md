# Processing Optimization of Large Dataset

This project focuses on optimizing a large customer dataset for *Training Data Ltd.*, an online data science training provider. The goal is to create a more efficient memory usage solution that enables faster processing and analysis, allowing predictive models to run on a reasonable timescale without reducing dataset size.

The dataset (`customer_train.csv`) includes anonymized student records, which will ultimately be used to predict if students are interested in new job opportunities. This prediction helps *Training Data Ltd.* target students with relevant career opportunities.

## Project Highlights
- **Data Cleaning**: Standardized text fields and handled missing values to ensure data integrity.
- **Memory Optimization**: Transformed data types (including categorical encoding, two-factor mapping, and numeric optimizations) to reduce memory usage by 98.78%.
- **Targeted Filtering**: Focused on high-value segments of the dataset for meaningful recruitment targeting.
- **Results**: Significant memory savings enhance processing efficiency and make predictive modeling faster and more practical.

This project demonstrates effective data management and transformation techniques to improve large-scale data efficiency, providing a practical solution for managing sizable datasets in predictive modeling.

## Contents
- [python_code.ipynb](https://github.com/randallcampher/processing-optimization/blob/main/python_code.ipynb): The data transformation and optimization process
- `README.md`: Overview of the project and key methods used

## Technologies Used
- Python
- Pandas

This repository showcases the complete workflow for transforming a large dataset into a leaner, more manageable format, ideal for applications in predictive analytics.
