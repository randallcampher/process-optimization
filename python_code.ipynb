{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the pandas library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the 'customer_train.csv' file into a pandas DataFrame\n",
    "# This will load the dataset from the specified CSV file into the 'ds_jobs_transformed' DataFrame\n",
    "ds_jobs_transformed = pd.read_csv('customer_train.csv')\n",
    "\n",
    "# Creating a copy of the DataFrame to avoid modifying the original dataset\n",
    "# The 'copy()' method ensures that changes made to 'ds_jobs_transformed' do not affect the original 'ds_jobs_transformed' DataFrame\n",
    "ds_jobs_transformed = ds_jobs_transformed.copy()\n",
    "\n",
    "# Displaying the first 5 rows of the DataFrame\n",
    "ds_jobs_transformed.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the .nunique() method to calculate the number of unique values in each column of the DataFrame.\n",
    "# This will return the count of distinct values for each column in the ds_jobs_transformed DataFrame.\n",
    "\n",
    "print(ds_jobs_transformed.nunique())  # Printing the output of unique value counts for each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the .info() method to display a summary of the DataFrame, including data types, non-null counts, and memory usage.\n",
    "# The 'memory_usage' argument is set to 'deep' to calculate detailed memory usage, including memory used by object (string) columns.\n",
    "# Without 'deep', pandas only estimates memory for object columns, but 'deep' considers the actual content size of those columns.\n",
    "\n",
    "ds_jobs_transformed.info(memory_usage='deep')  # Displaying detailed information about the DataFrame, including precise memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .sum() is used to add up the memory usage of all columns to get the total memory usage of the DataFrame in bytes.\n",
    "\n",
    "memory_df1 = ds_jobs_transformed.memory_usage(deep=True).sum()  # Storing the total memory usage of ds_jobs_transformed in the variable 'memory_df1'\n",
    "\n",
    "# The total memory usage (in bytes) is stored in memory_df1 and can be printed or further used for calculations.\n",
    "memory_df1  # Returning or displaying the total memory usage in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over each column in the DataFrame that has the data type 'object' (usually string or categorical data)\n",
    "for col in ds_jobs_transformed.select_dtypes('object'):\n",
    "   \n",
    "   # Converting all the values in the current column to lowercase using .str.lower().\n",
    "   # This ensures that the values are case-insensitive (e.g., 'YES' and 'yes' will become the same).\n",
    "   ds_jobs_transformed[col] = ds_jobs_transformed[col].str.lower()\n",
    "\n",
    "   # Getting the most frequent value (mode) in the current column.\n",
    "   # .mode() returns a Series of the most frequent values, but we only need the first one [0] since there might be multiple modes.\n",
    "   mode_values = ds_jobs_transformed[col].mode()[0]\n",
    "\n",
    "   # Replacing all NaN (missing) values in the current column with the mode (most frequent value).\n",
    "   # This ensures that missing values are filled with the most common value in the column.\n",
    "   ds_jobs_transformed[col] = ds_jobs_transformed[col].fillna(mode_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over each column in the DataFrame that has the data type 'int64' (integer data)\n",
    "for col in ds_jobs_transformed.select_dtypes('int64'):\n",
    "   \n",
    "   # Calculating the mean of the current column.\n",
    "   # .mean() computes the average of all non-null values in the column.\n",
    "   mean_values = ds_jobs_transformed[col].mean()\n",
    "\n",
    "   # Replacing all NaN (missing) values in the current column with the mean value.\n",
    "   # This ensures that missing values are filled with the average value of the column.\n",
    "   ds_jobs_transformed[col] = ds_jobs_transformed[col].fillna(mean_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first five rows of the ds_jobs_transformed DataFrame\n",
    "ds_jobs_transformed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_jobs_transformed.isnull().sum())# Calculating the number of missing (NaN) values in each column of the ds_jobs_transformed DataFrame.\n",
    "# .isnull() returns a DataFrame of the same shape with boolean values (True for NaN values and False for non-null values).\n",
    "# .sum() then counts the number of True values (NaN values) for each column, giving the total missing values in each column.\n",
    "\n",
    "print(ds_jobs_transformed.isnull().sum())  # Printing the count of missing values for each column in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the number of unique values in each column of the ds_jobs_transformed DataFrame.\n",
    "# .nunique() returns a Series where the index represents the column names and the values are the number of unique entries in each column.\n",
    "\n",
    "print(ds_jobs_transformed.nunique())  # Printing the count of unique values for each column in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dictionary called 'two_factor_map' that maps specific values in two columns to boolean values.\n",
    "# This dictionary will be used to convert certain categorical or numeric values into boolean (True/False) values.\n",
    "# The keys of the dictionary are the column names ('relevant_experience' and 'job_change'), and the values are the mapping rules.\n",
    "\n",
    "two_factor_map = {\n",
    "    # For the 'relevant_experience' column, the values 'No relevant experience' and 'Has relevant experience' are mapped to False and True, respectively.\n",
    "    'relevant_experience': {'No relevant experience': False, 'Has relevant experience': True},\n",
    "    \n",
    "    # For the 'job_change' column, the numeric values 0.0 and 1.0 are mapped to False and True, respectively.\n",
    "    'job_change': {0.0: False, 1.0: True}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dictionary called 'ordered_cats' that holds lists of ordered categorical values for specific columns.\n",
    "# These ordered categories will be used to convert the values in these columns into ordered categorical data types in the DataFrame.\n",
    "\n",
    "ordered_cats = {\n",
    "    # For the 'enrolled_university' column, the possible values are ordered from no enrollment to full-time course.\n",
    "    # This means 'no_enrollment' is the lowest and 'Full time course' is the highest.\n",
    "    'enrolled_university': ['no_enrollment', 'Part time course', 'Full time course'],\n",
    "    \n",
    "    # For the 'education_level' column, the education levels are ordered from Primary School to PhD.\n",
    "    'education_level': ['Primary School', 'High School', 'Graduate', 'Masters', 'Phd'],\n",
    "    \n",
    "    # For the 'experience' column, the experience ranges from '<1' year to '>20' years.\n",
    "    # A range from 1 to 20 years is created and concatenated with '<1' and '>20' at the ends.\n",
    "    'experience': ['<1'] + list(map(str, range(1, 21))) + ['>20'],\n",
    "    \n",
    "    # For the 'company_size' column, the company sizes are ordered from the smallest to the largest size.\n",
    "    # It starts from '<10' employees and goes to '10000+' employees.\n",
    "    'company_size': ['<10', '10-49', '50-99', '100-499', '500-999', '1000-4999', '5000-9999', '10000+'],\n",
    "    \n",
    "    # For the 'last_new_job' column, the possible values represent how long it has been since the person last changed jobs.\n",
    "    # The categories are ordered from 'never' to '>4' years since the last job change.\n",
    "    'last_new_job': ['never', '1', '2', '3', '4', '>4']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over each column in the ds_jobs_transformed DataFrame\n",
    "for col in ds_jobs_transformed:\n",
    "\n",
    "   # For columns 'relevant_experience' and 'job_change', map the values based on the two_factor_map dictionary.\n",
    "   # This converts specific values (e.g., 'No relevant experience' to False) into boolean (True/False) values.\n",
    "   if col in ['relevant_experience', 'job_change']:\n",
    "      ds_jobs_transformed[col] = ds_jobs_transformed[col].map(two_factor_map[col])\n",
    "\n",
    "   # For columns listed in the 'ordered_cats' dictionary, convert the column to an ordered categorical type.\n",
    "   # The 'pd.CategoricalDtype' method is used to create an ordered categorical type using the list of categories from 'ordered_cats'.\n",
    "   elif col in ordered_cats.keys():\n",
    "      category = pd.CategoricalDtype(ordered_cats[col], ordered=True)\n",
    "      ds_jobs_transformed[col] = ds_jobs_transformed[col].astype(category)\n",
    "\n",
    "   # For columns 'student_id' and 'training_hours', cast the column to 'int32' type.\n",
    "   # This is used for integer columns that do not require large number ranges.\n",
    "   elif col in ['student_id', 'training_hours']:\n",
    "      ds_jobs_transformed[col] = ds_jobs_transformed[col].astype('int32')\n",
    "\n",
    "   # For the 'city_development_index' column, cast it to 'float16', which saves memory for floating-point numbers.\n",
    "   elif col == 'city_development_index':\n",
    "      ds_jobs_transformed[col] = ds_jobs_transformed[col].astype('float16')\n",
    "\n",
    "   # For all other columns, convert them to 'category' type to optimize memory usage for categorical data.\n",
    "   else:\n",
    "      ds_jobs_transformed[col] = ds_jobs_transformed[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the ds_jobs_transformed DataFrame based on conditions applied to the 'experience' and 'company_size' columns.\n",
    "# This line keeps only the rows where 'experience' is greater than or equal to '10' years \n",
    "# and 'company_size' is greater than or equal to '1000-4999' employees.\n",
    "\n",
    "ds_jobs_transformed = ds_jobs_transformed[\n",
    "    (ds_jobs_transformed['experience'] >= '10') &  # Filtering for rows with 10 or more years of experience\n",
    "    (ds_jobs_transformed['company_size'] >= '1000-4999')  # Filtering for rows with company size between 1000 and 4999 employees\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the memory usage of the new ds_jobs_transformed DataFrame, including detailed memory usage for object columns (strings).\n",
    "# The 'memory_usage' method is used to get memory consumption for each column, and 'deep=True' ensures that the memory used by object data types is included.\n",
    "\n",
    "ds_jobs_transformed.info(memory_usage='deep')  # Prints the DataFrame's memory usage details, including the memory used by each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the memory usage of the ds_jobs_transformed DataFrame, including the memory used by object columns (strings).\n",
    "# The 'memory_usage' method is used to calculate the memory used by each column.\n",
    "# The 'deep=True' parameter ensures that the memory used by object types (e.g., strings) is fully accounted for.\n",
    "# .sum() is then used to aggregate the memory usage of all columns into a total value.\n",
    "\n",
    "memory_df2 = ds_jobs_transformed.memory_usage(deep=True).sum()  # Summing up the total memory usage of the entire DataFrame\n",
    "\n",
    "# Printing the total memory usage of the DataFrame after optimization\n",
    "memory_df2  # This will display the calculated memory usage value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in memory usage: 98.78 %\n"
     ]
    }
   ],
   "source": [
    "# Calculating the difference in memory usage as a percentage between two DataFrames' memory consumption.\n",
    "# The formula calculates how much the memory usage has decreased from memory_df1 to memory_df2.\n",
    "# It subtracts memory_df2 (the new memory usage) from memory_df1 (the old memory usage), divides by memory_df1, and multiplies by 100 to get the percentage reduction.\n",
    "\n",
    "memory_diff = (memory_df1 - memory_df2) / memory_df1 * 100  # Computing the percentage difference in memory usage\n",
    "\n",
    "# Printing the result, rounded to two decimal places.\n",
    "print('Difference in memory usage:', round(memory_diff, 2), '%')  # Displaying the memory usage difference as a percentage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
